{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a35361-eb52-4fb6-8dfd-537d9f39b4d5",
   "metadata": {},
   "source": [
    "# Assessing suitability of a location for thermal curtains and light shades to increase crop growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c81b13b-3a07-4fce-9297-fe562736444a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Plants do not grow effectively and get can get damaged in high temperatures and too much sun. Conversely dull, cool days inhibit photosynthesis and plants do not grow effeciently, if temperatures drop too low grow may be damaged and leaves dropped impacting growth for many weeks.  Many greenhouse growers choose to whitewash the glass to reflect the heat and dissipate the light. However, adding white wash is time consuming, so the wash is only applied once per season, meaning if a cooler or cloudier spell of weather occurs the plants can't grow optimally. An increasingly popular alternative is fitting greenhouses with shading curtains which are easier to remove on cloudier days and can been drawn at night to retain heat. Through this project we investigate areas of xxxxxxxxxxxxxxxx to see white washing and thermal curtains are not efficient due to frequent temperature variations.\n",
    "\n",
    "The meteostat platform has been used to access the weather data for the basis of this project [(1)](https://dev.meteostat.net/). Due to the amount of data it has been stored in a MySQL database and analysed using charts and predictions made using SciKitLearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1738f",
   "metadata": {},
   "source": [
    "# Plant shading techniques\n",
    "Each of the different plant shading techniques have different characteristics and efficiencies. White washing requires good knowledge on when to white wash and remove it, but is very cheap, thermal curtains are easier to use, but are more expensive to buy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e518395",
   "metadata": {},
   "source": [
    "## White washing\n",
    "White washing is best applied to green houses when the days are consistently above 25 degrees and removed at the end of the season. The white washing provides limited thermal properties at night and hence only day time highs are of importance. The standard white washing protocol reduces daytime temperatures by 6.8% on sunny days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2a3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "whitewash = 6.8/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2bb8e0",
   "metadata": {},
   "source": [
    "### Thermal curtains\n",
    "Thermal curtains can easily be pulled and retracted in line with local conditions. A shade rated for 50% shading gives the most similar effects in daytime to white washing and this reduced the daytime temperatures by 6.9% on sunny days and maintained over night temperatures by 2.1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b403c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "curtains_day = 6.9/100\n",
    "curtains_night = 2.1/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edee633",
   "metadata": {},
   "source": [
    "### The test plants\n",
    "\"Head lettuce is the most important salad vegetable gown in the United States\" [(2)](https://content.ces.ncsu.edu/lettuce). The optimum growing temperature for head lettuce is 15-18°C. At 20-26°C the plants start to flower and produce seed and need replacing and temperatures below 0°C will kill the plants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5caac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lettuce_min = 0\n",
    "lettuce_max = 20\n",
    "lettuce_range1 = 15\n",
    "lettuce_range2 = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89a628",
   "metadata": {},
   "source": [
    "Tomatoes are grown across the world and are an important staple in many cuisines. To increase their range and season many are grown in greenhouses. The optimal conditions for growth are 21-27°C with temperatures not dropping below 16°C or exceeding 29°C [(3)](https://drygair.com/blog/what-are-the-ideal-conditions-for-greenhouse-tomatoes/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91f4f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_min = 16\n",
    "tomato_max = 29\n",
    "tomato_range1 = 21\n",
    "tomato_range2 = 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e47271f-f5d4-460d-91d7-5a3ed9d34b9d",
   "metadata": {},
   "source": [
    "## Import needed tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38678af-59c2-45f7-b8be-9e490c370c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mySQL database\n",
    "import mysql.connector as msql\n",
    "\n",
    "# Import the data and unzip it\n",
    "import requests\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "# Create the dataframe and manipulate it\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e8be7",
   "metadata": {},
   "source": [
    "# Create a database to store the data\n",
    "Due to the large amounts of data needed for this analysis. A MySQL database needs to be created. MySQL offers the ability to store very large amounts of data in a structured and easily accessible way, where the data is persistent. \n",
    "\n",
    "To create the database we first set the connection parameters for the database and create the cursor to enable the connection for making SQL queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5091ec24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The database \"weather\" has been created\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters for the connection\n",
    "db = msql.connect(host='localhost', user='root',password='')\n",
    "\n",
    "# Create the cursor\n",
    "cursor = db.cursor()\n",
    "\n",
    "# Execute SQL query\n",
    "cursor.execute('CREATE DATABASE weather')\n",
    "print('The database \"weather\" has been created')\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8058df",
   "metadata": {},
   "source": [
    "## Create a table for the weather station data\n",
    "Within the database a table needs to be created for each of the data sources. The first table will hold a list of the weather stations [(4)](https://dev.meteostat.net/bulk/stations.html#endpoints). The data about all the weather stations is available at: https://bulk.meteostat.net/v2/stations/lite.json.gz. It is compressed using `gzip` so the file needs to be decompressed before creating the dataframe and inserting the information into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "484e9aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're connected to database: ('weather',)\n",
      "Creating table....\n",
      "Table is created....\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Connect to MySQL\n",
    "    con = msql.connect(host='localhost', database='weather', user='root', password='')\n",
    "\n",
    "    if con.is_connected():\n",
    "        cursor = con.cursor()\n",
    "        cursor.execute(\"SELECT DATABASE();\")\n",
    "        record = cursor.fetchone()\n",
    "        print(f\"You're connected to database: {record}\")\n",
    "        \n",
    "        # Drop the table if it exists\n",
    "        cursor.execute('DROP TABLE IF EXISTS stations_data')\n",
    "        print('Creating table....')\n",
    "        \n",
    "        # Create the table\n",
    "        sql = \"\"\"CREATE TABLE stations_data \n",
    "            (id VARCHAR(10) PRIMARY KEY, \n",
    "            name JSON, \n",
    "            country CHAR(2), \n",
    "            region VARCHAR(10), \n",
    "            national_id VARCHAR(10), \n",
    "            wmo_id VARCHAR(10), \n",
    "            icao_id VARCHAR(10), \n",
    "            iata_id VARCHAR(25), \n",
    "            latitude DECIMAL(9,6), \n",
    "            longitude DECIMAL(9,6), \n",
    "            elevation INT, \n",
    "            timezone VARCHAR(25),\n",
    "            history JSON, \n",
    "            hourly_start DATE, \n",
    "            hourly_end DATE, \n",
    "            daily_start DATE,\n",
    "            daily_end DATE,\n",
    "            monthly_start YEAR,\n",
    "            monthly_end YEAR,\n",
    "            normals_start YEAR,\n",
    "            normals_end YEAR\n",
    ")\"\"\"\n",
    "        cursor.execute(sql)\n",
    "        print(\"Table is created....\")\n",
    "\n",
    "except msql.Error as err:\n",
    "    print(err.msg)\n",
    "\n",
    "finally:\n",
    "    if con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd5357",
   "metadata": {},
   "source": [
    "## Read in the weather station data\n",
    "\n",
    "Then the weather stations table needs to be populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d4b0db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database.\n",
      "Data inserted successfully.\n",
      "MySQL connection is closed.\n"
     ]
    }
   ],
   "source": [
    "# File name and location\n",
    "url = \"https://bulk.meteostat.net/v2/stations/full.json.gz\"\n",
    "file_name = \"full.json.gz\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url, stream=True)\n",
    "with open(file_name, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Decompress the file\n",
    "with gzip.open(file_name, 'rt', encoding='utf-8') as gz_file:\n",
    "    stations_file = json.load(gz_file)\n",
    "\n",
    "# Connect to the MySQL Database\n",
    "try:\n",
    "    con = msql.connect(host='localhost', database='weather', user='root', password='')\n",
    "    if con.is_connected():\n",
    "        cursor = con.cursor()\n",
    "        print(\"Connected to the database.\")\n",
    "\n",
    "        # Prepare SQL statement for data insertion\n",
    "        sql_insert = \"\"\"\n",
    "            INSERT INTO stations_data (\n",
    "                id, name, country, region, national_id, wmo_id, icao_id, iata_id,\n",
    "                latitude, longitude, elevation, timezone, history,\n",
    "                hourly_start, hourly_end, daily_start, daily_end,\n",
    "                monthly_start, monthly_end, normals_start, normals_end\n",
    "            ) VALUES (\n",
    "                %(id)s, %(name)s, %(country)s, %(region)s, %(national_id)s, %(wmo_id)s, %(icao_id)s, %(iata_id)s,\n",
    "                %(latitude)s, %(longitude)s, %(elevation)s, %(timezone)s, %(history)s,\n",
    "                %(hourly_start)s, %(hourly_end)s, %(daily_start)s, %(daily_end)s,\n",
    "                %(monthly_start)s, %(monthly_end)s, %(normals_start)s, %(normals_end)s\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        # Insert Data into MySQL Table\n",
    "        for station in stations_file:\n",
    "            data = {\n",
    "                'id': station.get('id'),\n",
    "                'name': json.dumps(station.get('name', {})),  # Convert dictionary to JSON string\n",
    "                'country': station.get('country'),\n",
    "                'region': station.get('region'),\n",
    "                'national_id': station.get('national'),\n",
    "                'wmo_id': station.get('wmo'),\n",
    "                'icao_id': station.get('icao'),\n",
    "                'iata_id': station.get('iata'),\n",
    "                'latitude': station.get('latitude'),\n",
    "                'longitude': station.get('longitude'),\n",
    "                'elevation': station.get('elevation'),\n",
    "                'timezone': station.get('timezone'),\n",
    "                'history': json.dumps(station.get('history', [])),  # Convert list to JSON string\n",
    "                'hourly_start': station.get('inventory', {}).get('hourly', {}).get('start'),\n",
    "                'hourly_end': station.get('inventory', {}).get('hourly', {}).get('end'),\n",
    "                'daily_start': station.get('inventory', {}).get('daily', {}).get('start'),\n",
    "                'daily_end': station.get('inventory', {}).get('daily', {}).get('end'),\n",
    "                'monthly_start': station.get('inventory', {}).get('monthly', {}).get('start'),\n",
    "                'monthly_end': station.get('inventory', {}).get('monthly', {}).get('end'),\n",
    "                'normals_start': station.get('inventory', {}).get('normals', {}).get('start'),\n",
    "                'normals_end': station.get('inventory', {}).get('normals', {}).get('end'),\n",
    "            }\n",
    "            cursor.execute(sql_insert, data)\n",
    "\n",
    "        # Commit the transaction\n",
    "        con.commit()\n",
    "        print(\"Data inserted successfully.\")\n",
    "\n",
    "# Print details if an error occurs\n",
    "except msql.Error as err:\n",
    "    print(f\"Error: {err}\")\n",
    "\n",
    "# Close the connection\n",
    "finally:\n",
    "    if con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()\n",
    "        print(\"MySQL connection is closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf18e12f",
   "metadata": {},
   "source": [
    "## Select the country of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95da4b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The country codes are explained at: https://www.iban.com/country-codes\n",
      "The countries available are:\n",
      "('CA',)\n",
      "('NO',)\n",
      "('SE',)\n",
      "('FI',)\n",
      "('DE',)\n",
      "('GB',)\n",
      "('IM',)\n",
      "('IE',)\n",
      "('IS',)\n",
      "('GL',)\n",
      "('US',)\n",
      "('FO',)\n",
      "('DK',)\n",
      "('NL',)\n",
      "('XA',)\n",
      "('BE',)\n",
      "('FR',)\n",
      "('LU',)\n",
      "('CH',)\n",
      "('CN',)\n",
      "('LI',)\n",
      "('PT',)\n",
      "('ES',)\n",
      "('GI',)\n",
      "('CV',)\n",
      "('AU',)\n",
      "('AT',)\n",
      "('CZ',)\n",
      "('SK',)\n",
      "('PL',)\n",
      "('HU',)\n",
      "('RS',)\n",
      "('SI',)\n",
      "('HR',)\n",
      "('XK',)\n",
      "('BA',)\n",
      "('CS',)\n",
      "('MK',)\n",
      "('AL',)\n",
      "('RO',)\n",
      "('BG',)\n",
      "('IT',)\n",
      "('MT',)\n",
      "('GR',)\n",
      "('TR',)\n",
      "('CY',)\n",
      "('RU',)\n",
      "('EE',)\n",
      "('LV',)\n",
      "('LT',)\n",
      "('BY',)\n",
      "('KZ',)\n",
      "('UA',)\n",
      "('MD',)\n",
      "('KG',)\n",
      "('GE',)\n",
      "('AZ',)\n",
      "('AM',)\n",
      "('UZ',)\n",
      "('TM',)\n",
      "('TJ',)\n",
      "('SY',)\n",
      "('LB',)\n",
      "('IL',)\n",
      "('PS',)\n",
      "('JO',)\n",
      "('SA',)\n",
      "('QA',)\n",
      "('KW',)\n",
      "('IQ',)\n",
      "('IR',)\n",
      "('AF',)\n",
      "('BH',)\n",
      "('AE',)\n",
      "('OM',)\n",
      "('YE',)\n",
      "('PK',)\n",
      "('BD',)\n",
      "('IN',)\n",
      "('NP',)\n",
      "('LK',)\n",
      "('MV',)\n",
      "('MN',)\n",
      "('HK',)\n",
      "('MO',)\n",
      "('TW',)\n",
      "('KP',)\n",
      "('KR',)\n",
      "('JP',)\n",
      "('MM',)\n",
      "('TH',)\n",
      "('MY',)\n",
      "('MX',)\n",
      "('SG',)\n",
      "('VN',)\n",
      "('LA',)\n",
      "('KH',)\n",
      "('EH',)\n",
      "('MA',)\n",
      "('DZ',)\n",
      "('TN',)\n",
      "('LY',)\n",
      "('NE',)\n",
      "('ML',)\n",
      "('MR',)\n",
      "('SN',)\n",
      "('GM',)\n",
      "('GW',)\n",
      "('GN',)\n",
      "('SL',)\n",
      "('SH',)\n",
      "('AQ',)\n",
      "('ST',)\n",
      "('IO',)\n",
      "('RE',)\n",
      "('MU',)\n",
      "('TF',)\n",
      "('EG',)\n",
      "('SD',)\n",
      "('ET',)\n",
      "('ER',)\n",
      "('DJ',)\n",
      "('SO',)\n",
      "('UG',)\n",
      "('KE',)\n",
      "('TZ',)\n",
      "('SC',)\n",
      "('CD',)\n",
      "('RW',)\n",
      "('BI',)\n",
      "('CG',)\n",
      "('GA',)\n",
      "('CF',)\n",
      "('TD',)\n",
      "('GQ',)\n",
      "('CM',)\n",
      "('NG',)\n",
      "('BJ',)\n",
      "('TG',)\n",
      "('GH',)\n",
      "('BF',)\n",
      "('CI',)\n",
      "('LR',)\n",
      "('AO',)\n",
      "('KM',)\n",
      "('MG',)\n",
      "('MZ',)\n",
      "('ZM',)\n",
      "('MW',)\n",
      "('ZW',)\n",
      "('NA',)\n",
      "('BW',)\n",
      "('ZA',)\n",
      "('SZ',)\n",
      "('LS',)\n",
      "('BV',)\n",
      "('CO',)\n",
      "('PM',)\n",
      "('PG',)\n",
      "('BM',)\n",
      "('BS',)\n",
      "('TC',)\n",
      "('CU',)\n",
      "('KY',)\n",
      "('JM',)\n",
      "('HT',)\n",
      "('DO',)\n",
      "('HN',)\n",
      "('PR',)\n",
      "('VI',)\n",
      "('VG',)\n",
      "('BZ',)\n",
      "('GT',)\n",
      "('SV',)\n",
      "('NI',)\n",
      "('CR',)\n",
      "('PA',)\n",
      "('PF',)\n",
      "('AI',)\n",
      "('KN',)\n",
      "('AG',)\n",
      "('AN',)\n",
      "('GP',)\n",
      "('DM',)\n",
      "('MQ',)\n",
      "('LC',)\n",
      "('VC',)\n",
      "('BB',)\n",
      "('GD',)\n",
      "('TT',)\n",
      "('AW',)\n",
      "('VE',)\n",
      "('GY',)\n",
      "('SR',)\n",
      "('GF',)\n",
      "('BR',)\n",
      "('EC',)\n",
      "('PE',)\n",
      "('BO',)\n",
      "('CL',)\n",
      "('PY',)\n",
      "('UY',)\n",
      "('AR',)\n",
      "('GS',)\n",
      "('FK',)\n",
      "('FM',)\n",
      "('UM',)\n",
      "('MH',)\n",
      "('KI',)\n",
      "('PW',)\n",
      "('NZ',)\n",
      "('SB',)\n",
      "('NR',)\n",
      "('NU',)\n",
      "('VU',)\n",
      "('NC',)\n",
      "('TV',)\n",
      "('FJ',)\n",
      "('TK',)\n",
      "('AS',)\n",
      "('WS',)\n",
      "('TO',)\n",
      "('CK',)\n",
      "('PN',)\n",
      "('NF',)\n",
      "('ID',)\n",
      "('BN',)\n",
      "('TL',)\n",
      "('PH',)\n",
      "('XP',)\n",
      "('XI',)\n",
      "('GU',)\n",
      "('MS',)\n",
      "('BT',)\n"
     ]
    }
   ],
   "source": [
    "con = msql.connect(host='localhost', database='weather', user='root', password='')\n",
    "\n",
    "cursor = con.cursor()\n",
    "\n",
    "sql = \"SELECT DISTINCT country FROM stations_data\"\n",
    "\n",
    "cursor.execute(sql)\n",
    "results = cursor.fetchall()\n",
    "print(\"The country codes are explained at: https://www.iban.com/country-codes\")\n",
    "print(f\"The countries available are:\")\n",
    "for row in results:\n",
    "    print(row)\n",
    "\n",
    "cursor.close()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3646b-6f11-44ae-ad9c-edb6855cb5aa",
   "metadata": {},
   "source": [
    "## Prompt user to input the country of interest\n",
    "The user should be prompted to select the country of interest. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e6adf04-a565-42e2-98d7-195f885f6ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The country codes are explained at: https://www.iban.com/country-codes\n"
     ]
    }
   ],
   "source": [
    "print(\"The country codes are explained at: https://www.iban.com/country-codes\")\n",
    "chosencountry = input(str(\"Enter the country of interest, using the two digit code: \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1489e8d8-5b44-49e1-ab63-dc54bfe6a385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regions within CA available are:\n",
      "('AB',)\n",
      "('MB',)\n",
      "('BC',)\n",
      "('SK',)\n",
      "('NU',)\n",
      "('ON',)\n",
      "('YT',)\n",
      "('QC',)\n",
      "('NS',)\n",
      "('NL',)\n",
      "('NT',)\n",
      "('PE',)\n",
      "('NB',)\n",
      "('SD',)\n",
      "('NF',)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Connect to the database\n",
    "    con = msql.connect(host='localhost', database='weather', user='root', password='')\n",
    "    if con.is_connected():\n",
    "        cursor = con.cursor()\n",
    "        \n",
    "        # Get user to input the country code\n",
    "        chosencountry = input(str(\"Enter the country of interest, using the two digit code: \")).strip().upper()\n",
    "        \n",
    "        # Check the user input is valid\n",
    "        if len(chosencountry) != 2:\n",
    "            print(\"Invalid country code. Please enter a valid 2- character country code.\")\n",
    "        else: # Exectute the SQL query\n",
    "            query = sql = \"SELECT DISTINCT region FROM stations_data WHERE country = %s\"\n",
    "            cursor.execute(query, (chosencountry,)) \n",
    "            \n",
    "            # Fetch and display the results\n",
    "            results = cursor.fetchall()\n",
    "            if results:\n",
    "                print(f\"The regions within {chosencountry} available are:\")\n",
    "                for row in results:\n",
    "                    print(row)\n",
    "            else:\n",
    "                print(f\"No regions found for country code: {chosencountry}\")\n",
    "                \n",
    "except msql.Error as err:\n",
    "    print(err)\n",
    "\n",
    "finally:\n",
    "    if con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791f187-9843-4044-a28e-e18ca25e3094",
   "metadata": {},
   "source": [
    "## Select the weather stations in the country and region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d6b7885-247d-4b44-a3b5-b4c62a952796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stations within ON are:\n",
      "('0A6XG',)\n",
      "('0CNUO',)\n",
      "('0CO7B',)\n",
      "('0FV1F',)\n",
      "('0FV2W',)\n",
      "('10RIK',)\n",
      "('128HL',)\n",
      "('13178',)\n",
      "('13IQV',)\n",
      "('17CKT',)\n",
      "('1D16Y',)\n",
      "('1J1PJ',)\n",
      "('1JWST',)\n",
      "('1Y8OJ',)\n",
      "('21O3U',)\n",
      "('2DO61',)\n",
      "('2FXL8',)\n",
      "('2MDV9',)\n",
      "('2W8UZ',)\n",
      "('2XUGG',)\n",
      "('3EADS',)\n",
      "('3S56J',)\n",
      "('3XLPN',)\n",
      "('44BUQ',)\n",
      "('4DA9G',)\n",
      "('4DUJO',)\n",
      "('540Y1',)\n",
      "('5J9E9',)\n",
      "('5MKGL',)\n",
      "('5RUZT',)\n",
      "('65YVF',)\n",
      "('6N2T2',)\n",
      "('71063',)\n",
      "('71099',)\n",
      "('71151',)\n",
      "('71161',)\n",
      "('71171',)\n",
      "('71172',)\n",
      "('71193',)\n",
      "('71260',)\n",
      "('71261',)\n",
      "('71262',)\n",
      "('71263',)\n",
      "('71265',)\n",
      "('71268',)\n",
      "('71270',)\n",
      "('71272',)\n",
      "('71281',)\n",
      "('71282',)\n",
      "('71290',)\n",
      "('71291',)\n",
      "('71292',)\n",
      "('71294',)\n",
      "('71295',)\n",
      "('71296',)\n",
      "('71297',)\n",
      "('71298',)\n",
      "('71299',)\n",
      "('71300',)\n",
      "('71301',)\n",
      "('71303',)\n",
      "('71307',)\n",
      "('71309',)\n",
      "('71314',)\n",
      "('71352',)\n",
      "('71368',)\n",
      "('71369',)\n",
      "('71399',)\n",
      "('71416',)\n",
      "('71430',)\n",
      "('71431',)\n",
      "('71432',)\n",
      "('71433',)\n",
      "('71434',)\n",
      "('71435',)\n",
      "('71436',)\n",
      "('71437',)\n",
      "('71438',)\n",
      "('71439',)\n",
      "('71460',)\n",
      "('71462',)\n",
      "('71463',)\n",
      "('71464',)\n",
      "('71465',)\n",
      "('71466',)\n",
      "('71468',)\n",
      "('71508',)\n",
      "('71527',)\n",
      "('71534',)\n",
      "('71538',)\n",
      "('71559',)\n",
      "('71573',)\n",
      "('71581',)\n",
      "('71582',)\n",
      "('71615',)\n",
      "('71620',)\n",
      "('71621',)\n",
      "('71623',)\n",
      "('71624',)\n",
      "('71625',)\n",
      "('71628',)\n",
      "('71629',)\n",
      "('71630',)\n",
      "('71631',)\n",
      "('71632',)\n",
      "('71633',)\n",
      "('71637',)\n",
      "('71639',)\n",
      "('71642',)\n",
      "('71649',)\n",
      "('71655',)\n",
      "('71660',)\n",
      "('71667',)\n",
      "('71672',)\n",
      "('71676',)\n",
      "('71677',)\n",
      "('71694',)\n",
      "('71697',)\n",
      "('71704',)\n",
      "('71730',)\n",
      "('71731',)\n",
      "('71733',)\n",
      "('71735',)\n",
      "('71738',)\n",
      "('71739',)\n",
      "('71747',)\n",
      "('71749',)\n",
      "('71750',)\n",
      "('71751',)\n",
      "('71752',)\n",
      "('71755',)\n",
      "('71767',)\n",
      "('71820',)\n",
      "('71831',)\n",
      "('71832',)\n",
      "('71834',)\n",
      "('71835',)\n",
      "('71836',)\n",
      "('71841',)\n",
      "('71842',)\n",
      "('71844',)\n",
      "('71845',)\n",
      "('71846',)\n",
      "('71847',)\n",
      "('71850',)\n",
      "('71854',)\n",
      "('71956',)\n",
      "('71962',)\n",
      "('74399',)\n",
      "('75BU2',)\n",
      "('7F8N0',)\n",
      "('7FGUJ',)\n",
      "('7L5OL',)\n",
      "('7PXZ6',)\n",
      "('7Q717',)\n",
      "('7ZTNB',)\n",
      "('82JYP',)\n",
      "('86WK3',)\n",
      "('8BQ1F',)\n",
      "('8FXMK',)\n",
      "('8HNVP',)\n",
      "('8IOCM',)\n",
      "('8PJPB',)\n",
      "('8SQCP',)\n",
      "('8TSI8',)\n",
      "('8WZUG',)\n",
      "('93RHE',)\n",
      "('99541',)\n",
      "('99596',)\n",
      "('99597',)\n",
      "('99598',)\n",
      "('99599',)\n",
      "('99655',)\n",
      "('9CZZP',)\n",
      "('9GNQU',)\n",
      "('9H92X',)\n",
      "('9HC7K',)\n",
      "('9K6D9',)\n",
      "('9M4XD',)\n",
      "('9VXHT',)\n",
      "('9W5OW',)\n",
      "('A2AJ4',)\n",
      "('ADBC1',)\n",
      "('AE2A1',)\n",
      "('ATA0X',)\n",
      "('AZ8HR',)\n",
      "('BI2NY',)\n",
      "('BSZRK',)\n",
      "('BWXFA',)\n",
      "('CNR66',)\n",
      "('CTAL0',)\n",
      "('CTCK0',)\n",
      "('CTGT0',)\n",
      "('CTWN0',)\n",
      "('CUNNB',)\n",
      "('CWCG0',)\n",
      "('CWDD0',)\n",
      "('CXQT0',)\n",
      "('CYAT0',)\n",
      "('CYPO0',)\n",
      "('CYSP0',)\n",
      "('CYWR0',)\n",
      "('CZK6B',)\n",
      "('D454J',)\n",
      "('DA6QZ',)\n",
      "('DEFS1',)\n",
      "('DJGJG',)\n",
      "('DMZ7Z',)\n",
      "('DNYFH',)\n",
      "('DRPUK',)\n",
      "('E7R70',)\n",
      "('EEE27',)\n",
      "('ERRXP',)\n",
      "('FBWKK',)\n",
      "('FEWAG',)\n",
      "('FO7ZN',)\n",
      "('FT7VY',)\n",
      "('G69T8',)\n",
      "('GTRWE',)\n",
      "('GUUD7',)\n",
      "('GVXVM',)\n",
      "('GXNJO',)\n",
      "('H1GXB',)\n",
      "('HIIF6',)\n",
      "('HU1MD',)\n",
      "('HUXFM',)\n",
      "('I5WNC',)\n",
      "('IQUA1',)\n",
      "('IU5MI',)\n",
      "('J35KT',)\n",
      "('J4DLP',)\n",
      "('JIW6L',)\n",
      "('JTA6I',)\n",
      "('JU5SQ',)\n",
      "('JXPVQ',)\n",
      "('K6CAG',)\n",
      "('K7B4X',)\n",
      "('K9PO6',)\n",
      "('KI36S',)\n",
      "('KKCUP',)\n",
      "('KV2YK',)\n",
      "('KXNWG',)\n",
      "('L1YUU',)\n",
      "('L6N28',)\n",
      "('LH84B',)\n",
      "('LII7V',)\n",
      "('MAU7O',)\n",
      "('MEIQU',)\n",
      "('MRDYG',)\n",
      "('MVZI9',)\n",
      "('MYD1J',)\n",
      "('N4N88',)\n",
      "('N74GR',)\n",
      "('NLAUF',)\n",
      "('NUIKA',)\n",
      "('NVVAD',)\n",
      "('NZY2Y',)\n",
      "('O879F',)\n",
      "('OA9QZ',)\n",
      "('OH148',)\n",
      "('OILVL',)\n",
      "('OQ2DX',)\n",
      "('OTBSC',)\n",
      "('PGXRO',)\n",
      "('PH6SL',)\n",
      "('PWLAQ',)\n",
      "('Q4C6T',)\n",
      "('Q9WNN',)\n",
      "('QBXLB',)\n",
      "('QHA0T',)\n",
      "('QMGGX',)\n",
      "('QOHE2',)\n",
      "('R4X11',)\n",
      "('RHCUS',)\n",
      "('RHTHS',)\n",
      "('SEURC',)\n",
      "('SR36N',)\n",
      "('STTYJ',)\n",
      "('T46N0',)\n",
      "('T9LWH',)\n",
      "('TJVRW',)\n",
      "('TZOUX',)\n",
      "('UB8H9',)\n",
      "('UDYR4',)\n",
      "('UHVDI',)\n",
      "('UJHR7',)\n",
      "('ULXZV',)\n",
      "('V5792',)\n",
      "('VBZL5',)\n",
      "('VDZJ4',)\n",
      "('VKWI5',)\n",
      "('VXLJF',)\n",
      "('W3CDZ',)\n",
      "('WFT9Q',)\n",
      "('WK3SZ',)\n",
      "('WY6W9',)\n",
      "('WYIQW',)\n",
      "('X4ONG',)\n",
      "('X7GSO',)\n",
      "('XFBN2',)\n",
      "('XM44W',)\n",
      "('Y7KHS',)\n",
      "('Z3I2A',)\n",
      "('ZFS01',)\n",
      "('ZSOGU',)\n",
      "('ZWC6W',)\n",
      "('ZXCYV',)\n",
      "('ZYC17',)\n",
      "('ZZU3Y',)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Connect to the database\n",
    "    con = msql.connect(host='localhost', database='weather', user='root', password='')\n",
    "    if con.is_connected():\n",
    "        cursor = con.cursor()\n",
    "        \n",
    "        # Get user to input the region code\n",
    "        chosenregion = input(str(\"Enter the region of interest, using the two digit code: \")).strip().upper()\n",
    "        \n",
    "        # Check the user input is valid\n",
    "        if len(chosenregion) != 2:\n",
    "            print(\"Invalid region code. Please enter a valid 2- character region code.\")\n",
    "        else: # Execute the SQL query\n",
    "            query =\"SELECT id FROM stations_data WHERE country = %s AND region = %s\"\n",
    "            cursor.execute(query, (chosencountry, chosenregion,)) \n",
    "            \n",
    "            # Fetch and display the results\n",
    "            station_ids = cursor.fetchall()\n",
    "            if results:\n",
    "                print(f\"The stations within {chosenregion} are:\")\n",
    "                for row in station_ids:\n",
    "                    print(str(row))\n",
    "            else:\n",
    "                print(f\"No stations found for region code: {chosenregion} in country code: {chosencountry}\")\n",
    "                \n",
    "except msql.Error as err:\n",
    "    print(err)\n",
    "\n",
    "finally:\n",
    "    if con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efabd1a",
   "metadata": {},
   "source": [
    "## Create a table for the weather data for each region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab41ae",
   "metadata": {},
   "source": [
    "### Create table for the weather data for the chosen country and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3b54864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're connected to database: ('weather',)\n",
      "Table 'ca_on_data' dropped if it existed.\n",
      "Table 'ca_on_data' is created successfully.\n"
     ]
    }
   ],
   "source": [
    "table_name = f\"{chosencountry}_{chosenregion}_data\".lower()\n",
    "\n",
    "try:\n",
    "    # Connect to MySQL\n",
    "    con = msql.connect(host='localhost', database='weather', user='root', password='')\n",
    "\n",
    "    if con.is_connected():\n",
    "        cursor = con.cursor()\n",
    "        cursor.execute(\"SELECT DATABASE();\")\n",
    "        record = cursor.fetchone()\n",
    "        print(f\"You're connected to database: {record}\")\n",
    "        \n",
    "        # Drop the table if it exists\n",
    "        drop_query = f\"DROP TABLE IF EXISTS `{table_name}`\"\n",
    "        cursor.execute(drop_query)\n",
    "        print(f\"Table '{table_name}' dropped if it existed.\")\n",
    "        \n",
    "        # Create the table\n",
    "        create_query = f\"\"\"\n",
    "        CREATE TABLE `{table_name}` (\n",
    "            id VARCHAR(10) PRIMARY KEY, \n",
    "            station_id VARCHAR(10),\n",
    "            date DATE,\n",
    "            avg_temp DECIMAL(5,1), \n",
    "            min_temp DECIMAL(5,1),\n",
    "            max_temp DECIMAL(5,1),\n",
    "            month INT,\n",
    "            year YEAR\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_query)\n",
    "        print(f\"Table '{table_name}' is created successfully.\")\n",
    "\n",
    "except msql.Error as err:\n",
    "    print(err.msg)\n",
    "\n",
    "finally:\n",
    "    if con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79faab",
   "metadata": {},
   "source": [
    "Now the table needs to be populated with the data for each of the weather stations in the chosen country and region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18edffcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database.\n",
      "Processing station: ('0A6XG',)\n",
      "MySQL connection is closed.\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://bulk.meteostat.net/v2/daily/('0A6XG',).csv.gz",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://bulk.meteostat.net/v2/daily/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m---> 21\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Ensure the request was successful\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Read the CSV file into a DataFrame\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     stations_data \u001b[38;5;241m=\u001b[39m gzip\u001b[38;5;241m.\u001b[39mGzipFile(fileobj\u001b[38;5;241m=\u001b[39mBytesIO(response\u001b[38;5;241m.\u001b[39mcontent))\n",
      "File \u001b[1;32mc:\\Users\\kirst\\anaconda3\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1021\u001b[0m     )\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://bulk.meteostat.net/v2/daily/('0A6XG',).csv.gz"
     ]
    }
   ],
   "source": [
    "# Connect to the MySQL Database\n",
    "try:\n",
    "    con = msql.connect(host='localhost', database='weather', user='root', password='')\n",
    "    if con.is_connected():\n",
    "        cursor = con.cursor()\n",
    "        print(\"Connected to the database.\")\n",
    "\n",
    "        # Prepare SQL statement for data insertion\n",
    "        sql_insert = \"\"\"\n",
    "            INSERT INTO `{table_name}` (id, station_id, date, avg_temp, min_temp, max_temp, month, year) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                    \"\"\"\n",
    "\n",
    "        # Insert Data into MySQL Table\n",
    "        for station in station_ids:\n",
    "            station = str(station)\n",
    "            print(f\"Processing station: {station}\")\n",
    "            \n",
    "            # Download the file\n",
    "            url = f\"https://bulk.meteostat.net/v2/daily/{station}.csv.gz\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Ensure the request was successful\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Read the CSV file into a DataFrame\n",
    "                stations_data = gzip.GzipFile(fileobj=BytesIO(response.content))\n",
    "                df = pd.read_csv(stations_data, usecols=[0,1,2,3],names =['Date', 'tavg', 'tmin', 'tmax'], parse_dates =['Date'])\n",
    "                \n",
    "                # Add a station ID column\n",
    "                df['station_id'] = station\n",
    "                \n",
    "                # Add month and year columns\n",
    "                df[\"Year\"] = df[\"Date\"].dt.year\n",
    "                df[\"Month\"] = df[\"Date\"].dt.month\n",
    "                \n",
    "                for _, row in df.iterrows():\n",
    "                        insert_query = f\"\"\"\n",
    "                            INSERT INTO `{table_name}` \n",
    "                            (id, date, avg_temp, min_temp, max_temp, month, year) \n",
    "                            VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "                        \"\"\"\n",
    "                        cursor.execute(insert_query, (\n",
    "                            row[\"station_id\"],\n",
    "                            row[\"date\"],\n",
    "                            row.get(\"tavg\", None),\n",
    "                            row.get(\"tmin\", None),\n",
    "                            row.get(\"tmax\", None),\n",
    "                            pd.to_datetime(row[\"date\"]).month,\n",
    "                            pd.to_datetime(row[\"date\"]).year,\n",
    "                        ))\n",
    "                con.commit()\n",
    "                print(f\"Data from station {station} inserted successfully.\")\n",
    "            else:\n",
    "                print(f\"Failed to download data for station: {station}\")\n",
    "except msql.Error as err:\n",
    "    print(f\"Error: {err}\")\n",
    "            \n",
    "# Close the connection\n",
    "finally:\n",
    "    if con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()\n",
    "        print(\"MySQL connection is closed.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdfe9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns for the year and month of the data\n",
    "for station in station_ids:\n",
    "    url = \"https://bulk.meteostat.net/v2/daily/\" + station + \".csv.gz\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "    # Decompress the gzip file\n",
    "    data = gzip.GzipFile(fileobj=BytesIO(response.content))\n",
    "\n",
    "    # Read in the CSV file\n",
    "    df = pd.read_csv(data, usecols=[0,1,2,3],names =['Date', 'tavg', 'tmin', 'tmax'], parse_dates =['Date'])\n",
    "d\n",
    "print(df.info())\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c52c3a-a81f-4f5d-8424-445a496eccc0",
   "metadata": {},
   "source": [
    "## Weather station data for weather station 1211\n",
    "https://dev.meteostat.net/bulk/daily.html#endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453b4ea-c8f6-4fc5-9ecd-34c9b93e71ab",
   "metadata": {},
   "source": [
    "## Average the data by month\n",
    "First check if there is any missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a674a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "count_nan = df.isnull().sum()\n",
    "print(count_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc51d47",
   "metadata": {},
   "source": [
    "If there is missing data use `dropna` to remove those rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8a2f3-f2e1-4402-af6b-42ac1a1b1b96",
   "metadata": {},
   "source": [
    "### Plot the data for the average temperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a57f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Group by year and month, then calculate the average temperature for each month\n",
    "df = df.dropna(subset=[\"tavg\"])  # Remove rows with missing average temperature\n",
    "monthly_avg = df.groupby([\"year\", \"month\"])[\"tavg\"].mean().reset_index()\n",
    "\n",
    "# Step 4: Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "years = monthly_avg[\"year\"].unique()\n",
    "\n",
    "for year in years:\n",
    "    year_data = monthly_avg[monthly_avg[\"year\"] == year]\n",
    "    plt.plot(year_data[\"month\"], year_data[\"tavg\"], marker=\"o\", label=str(year))\n",
    "\n",
    "\n",
    "plt.title(\"Average Monthly Temperature\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Average Temperature (°C)\")\n",
    "plt.xticks(range(1, 13), [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"])\n",
    "plt.legend(title=\"Year\", loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d66b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Group by year and month, then calculate the average temperature for each month\n",
    "df = df.dropna(subset=[\"tmax\"])  # Remove rows with missing average temperature\n",
    "monthly_avg = df.groupby([\"year\", \"month\"])[\"tmax\"].mean().reset_index()\n",
    "\n",
    "# Step 4: Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "years = monthly_avg[\"year\"].unique()\n",
    "\n",
    "for year in years:\n",
    "    year_data = monthly_avg[monthly_avg[\"year\"] == year]\n",
    "    plt.plot(year_data[\"month\"], year_data[\"tmax\"], marker=\"o\", label=str(year))\n",
    "\n",
    "\n",
    "# Add horizontal lines at 23°C and 15°C\n",
    "plt.axhline(y=23, color='red', linestyle='--', linewidth=1, label='23°C')\n",
    "plt.axhline(y=15, color='blue', linestyle='--', linewidth=1, label='15°C')\n",
    "\n",
    "\n",
    "plt.title(\"Maximum Monthly Temperature\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Maximum Monthly Temperature (°C)\")\n",
    "plt.xticks(range(1, 13), [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"])\n",
    "plt.legend(title=\"Year\", loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_avg.plot(x='Date', kind='line')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f1784-5daa-440d-8fcc-3bef5f7194ba",
   "metadata": {},
   "source": [
    "## Regression and Scikit Learn\n",
    "https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b009db-c438-4cb1-986f-fabca8031ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load Data\n",
    "url = \"https://bulk.meteostat.net/v2/daily/0CNUO.csv.gz\"\n",
    "df = pd.read_csv(url, compression='gzip', usecols=[0,1,2,3], names =[\"date\", \"tavg\", \"tmin\", \"tmax\"])\n",
    "\n",
    "# Step 2: Select Relevant Columns\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 3: Prepare Features\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Features: Day of the year and year; Target: Average temperature\n",
    "X = df[['day_of_year', 'year']]\n",
    "y = df['tavg']\n",
    "\n",
    "# Step 4: Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train Model\n",
    "model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate Model\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}°C\")\n",
    "\n",
    "# Step 7: Predict for Next 5 Years\n",
    "future_years = pd.date_range(start=df['date'].max() + timedelta(days=1), \n",
    "                             periods=5 * 365, freq='D')\n",
    "future_df = pd.DataFrame({\n",
    "    'date': future_years,\n",
    "    'day_of_year': future_years.day_of_year,\n",
    "    'year': future_years.year\n",
    "})\n",
    "future_predictions = model.predict(future_df[['day_of_year', 'year']])\n",
    "future_df['predicted_tavg'] = future_predictions\n",
    "\n",
    "# Step 8: Plot Known Data and Predictions\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot historical data\n",
    "plt.plot(df['date'], df['tavg'], label='Historical Data', alpha=0.6)\n",
    "\n",
    "# Plot predictions\n",
    "plt.plot(future_df['date'], future_df['predicted_tavg'], label='Predictions', alpha=0.8)\n",
    "\n",
    "plt.title('Historical and Predicted Average Temperatures')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Temperature (°C)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Step 9: Save Predictions\n",
    "future_df.to_csv('predicted_temperatures.csv', index=False)\n",
    "print(f\"Predictions saved to 'predicted_temperatures.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c13336-0325-4cc4-9e46-cead968cff74",
   "metadata": {},
   "source": [
    "# Sci Kit Learn\n",
    "https://www.askpython.com/python/examples/weather-data-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18df94c-780c-4643-a470-959f8c41b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import cycle, islice\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7181a2-bdd6-4b9d-a9a4-48353aba4769",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9dc76-7457-4ea1-99c2-ceed3e91bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"Date\"\n",
    "tavg = \"Average air temperature in Celsius\"\n",
    "tmin = \"Minimum air temperature in Celsius\"\n",
    "tmax = \"Maximum air temperature in Celsius\"\n",
    "\n",
    "station1211 = pd.read_csv(\"https://bulk.meteostat.net/v2/daily/0CNUO.csv.gz\", usecols = [0,1,2,3], names =[date, tavg, tmin, tmax],parse_dates=[0])\n",
    "#station1211[\"id\"] = df.index + 1\n",
    "print(station1211.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d213e6-c62a-4f89-a467-992cee446990",
   "metadata": {},
   "source": [
    "## Sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb102c5-cf80-4b77-b35f-0ed476c46ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = station1211[(station1211['Date'] % 2) == 0]\n",
    "sampled_df.shape\n",
    "\n",
    "#sampled_df.set_index('Date', inplace=True)\n",
    "\n",
    "# del sampled_df['Average air temperature in Celsius']\n",
    "# del sampled_df['Maximum air temperature in Celsius']\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf863f-aef4-4244-a812-5dd695609f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = sampled_df.shape[0]\n",
    "sampled_df = sampled_df.dropna()\n",
    "A = sampled_df.shape[0]\n",
    " \n",
    "print(\"No of rows deleted: \", B-A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0019962e-765b-44c7-8dd3-329a9fd6dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Load Data\n",
    "url = \"https://bulk.meteostat.net/v2/daily/0CNUO.csv.gz\"\n",
    "df = pd.read_csv(url, compression='gzip', usecols=[0,1,2,3], names =[date, tavg, tmin, tmax])\n",
    "df.info()\n",
    "\n",
    "# Step 2: Select First Four Columns\n",
    "#df = df[0,1,2,3]\n",
    "\n",
    "# Step 3: Handle Missing Values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 4: Add Cyclical Features for Date\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['day_of_year'] = df['Date'].dt.dayofyear\n",
    "df['sin_day'] = np.sin(2 * np.pi * df['day_of_year'] / 365.0)\n",
    "df['cos_day'] = np.cos(2 * np.pi * df['day_of_year'] / 365.0)\n",
    "\n",
    "# Step 5: Prepare Data for Clustering\n",
    "features = ['tavg', 'tmin', 'tmax', 'sin_day', 'cos_day']\n",
    "X = df[features]\n",
    "\n",
    "# Step 6: Apply KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Step 7: Visualize Clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in range(kmeans.n_clusters):\n",
    "    cluster_data = df[df['cluster'] == cluster]\n",
    "    plt.scatter(cluster_data['day_of_year'], cluster_data['tavg'], label=f'Cluster {cluster}')\n",
    "    \n",
    "plt.title('KMeans Clustering of Weather Data')\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Average Temperature (°C)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3545664-8281-42a7-a005-4a0f18c3faf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2affdaa4",
   "metadata": {},
   "source": [
    "## Thermoreactive glass\n",
    "\n",
    "The data in the below has been changed to help protect the intellectual property. Many plant crops scorch and reduce their efficiency at temperatures over 23 degrees and the traditional approach has been to whitewash growhouses. Unfortunately, this white washing is not easily removed, so whitewashes are applied at the beginning of the summer and washed off at the end, this means if there is a run of poor weather in summer the plants can't be productive and if there is good weather outside of summer the plants scorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60361ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight data for days with tmax > 23 degrees and proximity to tmax < 17 degrees\n",
    "# Step 5: Identify days with tmax > 23 degrees\n",
    "df[\"hot_days\"] = df[\"tmax\"] > 23\n",
    "\n",
    "# Step 6: Identify days within 5 days of a day with tmax < 17 degrees\n",
    "df[\"cold_days\"] = df[\"tmax\"] < 17\n",
    "df[\"cold_within_5\"] = df[\"cold_days\"].rolling(window=5, min_periods=1).max().shift(-4).fillna(0).astype(bool)\n",
    "\n",
    "df[\"hot_near_cold\"] = df[\"hot_days\"] & df[\"cold_within_5\"]\n",
    "\n",
    "# Step 7: Group by month and count hot days and hot days near cold days\n",
    "highlight_summary = df.groupby(\"month\").agg(\n",
    "    total_hot_days=(\"hot_days\", \"sum\"),\n",
    "    hot_near_cold_days=(\"hot_near_cold\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "# Highlight data for days with tmax > 23 degrees and proximity to tmax < 17 degrees\n",
    "# Step 5: Identify days with tmax > 23 degrees\n",
    "df[\"hot_days\"] = df[\"tmax\"] > 23\n",
    "\n",
    "# Step 6: Identify days within 5 days of a day with tmax < 17 degrees\n",
    "df[\"cold_days\"] = df[\"tmax\"] < 17\n",
    "df[\"cold_within_5\"] = df[\"cold_days\"].rolling(window=5, min_periods=1).max().shift(-4).fillna(0).astype(bool)\n",
    "\n",
    "df[\"hot_near_cold\"] = df[\"hot_days\"] & df[\"cold_within_5\"]\n",
    "\n",
    "# Step 7: Group by year and month to count hot days and hot days near cold days\n",
    "highlight_summary = df.groupby([\"year\", \"month\"]).agg(\n",
    "    total_hot_days=(\"hot_days\", \"sum\"),\n",
    "    hot_near_cold_days=(\"hot_near_cold\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "# Step 8: Plot hot days and hot days near cold days\n",
    "plt.figure(figsize=(12, 6))\n",
    "for year in highlight_summary[\"year\"].unique():\n",
    "    year_data = highlight_summary[highlight_summary[\"year\"] == year]\n",
    "    plt.plot(year_data[\"month\"], year_data[\"total_hot_days\"], marker=\"o\", label=f\"Hot Days {year}\")\n",
    "    plt.plot(year_data[\"month\"], year_data[\"hot_near_cold_days\"], marker=\"x\", label=f\"Hot Near Cold {year}\")\n",
    "\n",
    "plt.title(\"Monthly Hot Days and Hot Near Cold Days\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of Days\")\n",
    "plt.xticks(range(1, 13), [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"])\n",
    "plt.legend(title=\"Year\", loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Step 9: Predict trends for the next five years\n",
    "# Prepare data for regression analysis\n",
    "highlight_summary[\"time_index\"] = highlight_summary[\"year\"] + (highlight_summary[\"month\"] - 1) / 12.0\n",
    "\n",
    "# Predict total_hot_days\n",
    "X = highlight_summary[[\"time_index\"]]\n",
    "y_hot_days = highlight_summary[\"total_hot_days\"]\n",
    "hot_days_model = LinearRegression().fit(X, y_hot_days)\n",
    "\n",
    "# Predict hot_near_cold_days\n",
    "y_hot_near_cold_days = highlight_summary[\"hot_near_cold_days\"]\n",
    "hot_near_cold_model = LinearRegression().fit(X, y_hot_near_cold_days)\n",
    "\n",
    "# Generate future predictions\n",
    "future_years = np.arange(highlight_summary[\"year\"].max() + 1, highlight_summary[\"year\"].max() + 6)\n",
    "future_months = np.tile(range(1, 13), len(future_years))\n",
    "future_years_repeated = np.repeat(future_years, 12)\n",
    "future_time_index = future_years_repeated + (future_months - 1) / 12.0\n",
    "\n",
    "future_X = pd.DataFrame({\"time_index\": future_time_index})\n",
    "future_hot_days = hot_days_model.predict(future_X)\n",
    "future_hot_near_cold_days = hot_near_cold_model.predict(future_X)\n",
    "\n",
    "# Combine predictions into a DataFrame\n",
    "future_predictions = pd.DataFrame({\n",
    "    \"year\": future_years_repeated,\n",
    "    \"month\": future_months,\n",
    "    \"predicted_hot_days\": future_hot_days,\n",
    "    \"predicted_hot_near_cold_days\": future_hot_near_cold_days\n",
    "})\n",
    "\n",
    "# Display predictions\n",
    "print(future_predictions)\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "for year in future_years:\n",
    "    year_data = future_predictions[future_predictions[\"year\"] == year]\n",
    "    plt.plot(year_data[\"month\"], year_data[\"predicted_hot_days\"], marker=\"o\", label=f\"Predicted Hot Days {year}\")\n",
    "    plt.plot(year_data[\"month\"], year_data[\"predicted_hot_near_cold_days\"], marker=\"x\", label=f\"Predicted Hot Near Cold {year}\")\n",
    "\n",
    "plt.title(\"Predicted Monthly Hot Days and Hot Near Cold Days\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of Days\")\n",
    "plt.xticks(range(1, 13), [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"])\n",
    "plt.legend(title=\"Year\", loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d63a85",
   "metadata": {},
   "source": [
    "[(1)]https://dev.meteostat.net/\n",
    "[(2)] https://content.ces.ncsu.edu/lettuce\n",
    "[(3)] https://drygair.com/blog/what-are-the-ideal-conditions-for-greenhouse-tomatoes/\n",
    "https://www.askpython.com/python/examples/weather-data-clustering\n",
    "\n",
    "https://esciencecenter-digital-skills.github.io/lesson-machine-learning-intro/02-data-exploration.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
