{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a35361-eb52-4fb6-8dfd-537d9f39b4d5",
   "metadata": {},
   "source": [
    "# Assessing suitability of a location for thermal curtains and light shades to increase crop growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c81b13b-3a07-4fce-9297-fe562736444a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Plants do not grow effectively and get can get damaged in high temperatures and too much sun. Conversely dull, cool days inhibit photosynthesis and plants do not grow effeciently, if temperatures drop too low grow may be damaged and leaves dropped impacting growth for many weeks.  Many greenhouse growers choose to whitewash the glass to reflect the heat and dissipate the light. However, adding white wash is time consuming, so the wash is only applied once per season, meaning if a cooler or cloudier spell of weather occurs the plants can't grow optimally. An increasingly popular alternative is fitting greenhouses with shading curtains which are easier to remove on cloudier days and can been drawn at night to retain heat. Through this project we investigate areas of xxxxxxxxxxxxxxxx to see white washing and thermal curtains are not efficient due to frequent temperature variations.\n",
    "\n",
    "The meteostat platform has been used to access the weather data for the basis of this project [(1)](https://dev.meteostat.net/). Due to the amount of data it has been stored in a MySQL database and analysed using charts and predictions made using SciKitLearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1738f",
   "metadata": {},
   "source": [
    "# Plant shading techniques\n",
    "Each of the different plant shading techniques have different characteristics and efficiencies. White washing requires good knowledge on when to white wash and remove it, but is very cheap, thermal curtains are easier to use, but are more expensive to buy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e518395",
   "metadata": {},
   "source": [
    "## White washing\n",
    "White washing is best applied to green houses when the days are consistently above 25 degrees and removed at the end of the season. The white washing provides limited thermal properties at night and hence only day time highs are of importance. The standard white washing protocol reduces daytime temperatures by 6.8% on sunny days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad2a3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "whitewash = 6.8/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2bb8e0",
   "metadata": {},
   "source": [
    "### Thermal curtains\n",
    "Thermal curtains can easily be pulled and retracted in line with local conditions. A shade rated for 50% shading gives the most similar effects in daytime to white washing and this reduced the daytime temperatures by 6.9% on sunny days and maintained over night temperatures by 2.1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b403c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "curtains_day = 6.9/100\n",
    "curtains_night = 2.1/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edee633",
   "metadata": {},
   "source": [
    "### The test plants\n",
    "\"Head lettuce is the most important salad vegetable gown in the United States\" [(2)](https://content.ces.ncsu.edu/lettuce). The optimum growing temperature for head lettuce is 15-18°C. At 20-26°C the plants start to flower and produce seed and need replacing and temperatures below 0°C will kill the plants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5caac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lettuce_min = 0\n",
    "lettuce_max = 20\n",
    "lettuce_range1 = 15\n",
    "lettuce_range2 = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89a628",
   "metadata": {},
   "source": [
    "Tomatoes are grown across the world and are an important staple in many cuisines. To increase their range and season many are grown in greenhouses. The optimal conditions for growth are 21-27°C with temperatures not dropping below 16°C or exceeding 29°C [(3)](https://drygair.com/blog/what-are-the-ideal-conditions-for-greenhouse-tomatoes/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91f4f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_min = 16\n",
    "tomato_max = 29\n",
    "tomato_range1 = 21\n",
    "tomato_range2 = 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e47271f-f5d4-460d-91d7-5a3ed9d34b9d",
   "metadata": {},
   "source": [
    "## Import needed tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a38678af-59c2-45f7-b8be-9e490c370c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mySQL database\n",
    "import mysql.connector as msql\n",
    "\n",
    "# Import the data and unzip it\n",
    "import requests\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "# Create the dataframe and manipulate it\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e8be7",
   "metadata": {},
   "source": [
    "# Create a database to store the data\n",
    "Due to the large amounts of data needed for this analysis. A MySQL database needs to be created. MySQL offers the ability to store very large amounts of data in a structured and easily accessible way, where the data is persistent. \n",
    "\n",
    "To create the database we first set the connection parameters for the database and create the cursor to enable the connection for making SQL queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5091ec24",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Set the parameters for the connection\n",
    "db = msql.connect(host='localhost', user='root',password='')\n",
    "\n",
    "# Create the cursor\n",
    "cursor = db.cursor()\n",
    "\n",
    "# Execute SQL query\n",
    "cursor.execute('CREATE DATABASE weather')\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8058df",
   "metadata": {},
   "source": [
    "Within the database a table needs to be created for each of the data sources. The first table will hold a list of the weather stations [(4)](https://dev.meteostat.net/bulk/stations.html#endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Connect to MySQL\n",
    "    con = msql.connect(host='localhost', database='weather', user='root', password='')\n",
    "\n",
    "    if con.is_connected():\n",
    "        cursor = con.cursor()\n",
    "        cursor.execute(\"SELECT DATABASE();\")\n",
    "        record = cursor.fetchone()\n",
    "        print(f\"You're connected to database: {record}\")\n",
    "        \n",
    "        # Drop the table if it exists\n",
    "        cursor.execute('DROP TABLE IF EXISTS stations')\n",
    "        print('Creating table....')\n",
    "        \n",
    "        # Create the table\n",
    "        sql = \"\"\"CREATE TABLE stations \n",
    "            (id VARCHAR(10) PRIMARY KEY, \n",
    "            name JSON, \n",
    "            country CHAR(2), \n",
    "            region VARCHAR(10), \n",
    "            national_id VARCHAR(10), \n",
    "            wmo_id VARCHAR(10), \n",
    "            icao_id VARCHAR(10), \n",
    "            iata_id VARCHAR(25), \n",
    "            latitude DECIMAL(9,6), \n",
    "            longitude DECIMAL(9,6), \n",
    "            elevation INT, \n",
    "            timezone VARCHAR(25),\n",
    "            history JSON, \n",
    "            hourly_start DATE, \n",
    "            hourly_end DATE, \n",
    "            daily_start DATE,\n",
    "            daily_end DATE,\n",
    "            monthly_start YEAR,\n",
    "            monthly_end YEAR,\n",
    "            normals_start YEAR,\n",
    "            normals_end YEAR\n",
    ")\"\"\"\n",
    "        cursor.execute(sql)\n",
    "        print(\"Table is created....\")\n",
    "\n",
    "except msql.Error as err:\n",
    "    print(err.msg)\n",
    "\n",
    "finally:\n",
    "    if con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd5357",
   "metadata": {},
   "source": [
    "Then the weather stations table needs to be populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b0db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download and Decompress the Data\n",
    "url = \"https://bulk.meteostat.net/v2/stations/full.json.gz\"\n",
    "file_name = \"full.json.gz\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url, stream=True)\n",
    "with open(file_name, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Decompress the file\n",
    "with gzip.open(file_name, 'rt', encoding='utf-8') as gz_file:\n",
    "    stations_data = json.load(gz_file)\n",
    "\n",
    "# Step 2: Connect to the MySQL Database\n",
    "try:\n",
    "    con = msql.connect(host='localhost', database='weather', user='root', password='')\n",
    "    if con.is_connected():\n",
    "        cursor = con.cursor()\n",
    "        print(\"Connected to the database.\")\n",
    "\n",
    "        # Prepare SQL statement for data insertion\n",
    "        sql_insert = \"\"\"\n",
    "            INSERT INTO stations (\n",
    "                id, name, country, region, national_id, wmo_id, icao_id, iata_id,\n",
    "                latitude, longitude, elevation, timezone, history,\n",
    "                hourly_start, hourly_end, daily_start, daily_end,\n",
    "                monthly_start, monthly_end, normals_start, normals_end\n",
    "            ) VALUES (\n",
    "                %(id)s, %(name)s, %(country)s, %(region)s, %(national_id)s, %(wmo_id)s, %(icao_id)s, %(iata_id)s,\n",
    "                %(latitude)s, %(longitude)s, %(elevation)s, %(timezone)s, %(history)s,\n",
    "                %(hourly_start)s, %(hourly_end)s, %(daily_start)s, %(daily_end)s,\n",
    "                %(monthly_start)s, %(monthly_end)s, %(normals_start)s, %(normals_end)s\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 3: Insert Data into MySQL Table\n",
    "        for station in stations_data:\n",
    "            data = {\n",
    "                'id': station.get('id'),\n",
    "                'name': json.dumps(station.get('name', {})),  # Convert dictionary to JSON string\n",
    "                'country': station.get('country'),\n",
    "                'region': station.get('region'),\n",
    "                'national_id': station.get('national'),\n",
    "                'wmo_id': station.get('wmo'),\n",
    "                'icao_id': station.get('icao'),\n",
    "                'iata_id': station.get('iata'),\n",
    "                'latitude': station.get('latitude'),\n",
    "                'longitude': station.get('longitude'),\n",
    "                'elevation': station.get('elevation'),\n",
    "                'timezone': station.get('timezone'),\n",
    "                'history': json.dumps(station.get('history', [])),  # Convert list to JSON string\n",
    "                'hourly_start': station.get('inventory', {}).get('hourly', {}).get('start'),\n",
    "                'hourly_end': station.get('inventory', {}).get('hourly', {}).get('end'),\n",
    "                'daily_start': station.get('inventory', {}).get('daily', {}).get('start'),\n",
    "                'daily_end': station.get('inventory', {}).get('daily', {}).get('end'),\n",
    "                'monthly_start': station.get('inventory', {}).get('monthly', {}).get('start'),\n",
    "                'monthly_end': station.get('inventory', {}).get('monthly', {}).get('end'),\n",
    "                'normals_start': station.get('inventory', {}).get('normals', {}).get('start'),\n",
    "                'normals_end': station.get('inventory', {}).get('normals', {}).get('end'),\n",
    "            }\n",
    "            cursor.execute(sql_insert, data)\n",
    "\n",
    "        # Commit the transaction\n",
    "        con.commit()\n",
    "        print(\"Data inserted successfully.\")\n",
    "\n",
    "except msql.Error as err:\n",
    "    print(f\"Error: {err}\")\n",
    "\n",
    "finally:\n",
    "    if con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()\n",
    "        print(\"MySQL connection is closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf18e12f",
   "metadata": {},
   "source": [
    "## Select the country of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da4b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = msql.connect(host='localhost', database='weather', user='root', password='')\n",
    "\n",
    "countries = pd.read_sql_query(\"SELECT DISTINCT country FROM stations\",\n",
    "    con)\n",
    "print(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efabd1a",
   "metadata": {},
   "source": [
    "## Create a table for the weather data for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25f671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b54864",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Connect to MySQL\n",
    "    con = msql.connect(host='localhost', database='weather', user='root', password='')\n",
    "\n",
    "    if con.is_connected():\n",
    "        cursor = con.cursor()\n",
    "        cursor.execute(\"SELECT DATABASE();\")\n",
    "        record = cursor.fetchone()\n",
    "        print(f\"You're connected to database: {record}\")\n",
    "        \n",
    "        # Drop the table if it exists\n",
    "        cursor.execute('DROP TABLE IF EXISTS stations')\n",
    "        print('Creating table....')\n",
    "        \n",
    "        cursor.execute('DROP TABLE IF EXISTS ontario_data')\n",
    "        print('Creating table....')\n",
    "        cursor.execute(\"CREATE TABLE ontario_data(station_ID varchar(8), date TIMESTAMP, average_temp float, min_temp float, max_temp float, month int, year int)\")\n",
    "        print(\"Table is created....\")\n",
    "\n",
    "except msql.Error as err:\n",
    "    print(err.msg)\n",
    "\n",
    "finally:\n",
    "    if con.is_connected():\n",
    "        cursor.close()\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03115e6-5042-4f24-b518-5c751bb8ba60",
   "metadata": {},
   "source": [
    "## Read in the weather station data\n",
    "The data about all the weather stations is available at: https://bulk.meteostat.net/v2/stations/lite.json.gz. It is compressed using `gzip` so they file needs to be decompressed before creating the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad582f1-efd7-4add-a1ab-294653bdb97d",
   "metadata": {},
   "source": [
    "### Create variables to make it easier to choose the country and region of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4d0be",
   "metadata": {},
   "source": [
    "host = os.getenv('MYSQL_HOST')\n",
    "port = os.getenv('MYSQL_PORT')\n",
    "user = os.getenv('MYSQL_USER')\n",
    "password = os.getenv('MYSQL_PASSWORD')\n",
    "database = os.getenv('MYSQL_DATABASE')\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=host,\n",
    "    port=int(port),\n",
    "    user=user,\n",
    "    passwd=password,\n",
    "    db=database,\n",
    "    charset='utf8mb4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f450c211-2b55-439d-9909-bcf55c04b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = df['country'].unique()\n",
    "print(f\"The countries within the file are:\\n {country}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3646b-6f11-44ae-ad9c-edb6855cb5aa",
   "metadata": {},
   "source": [
    "## Prompt user to request the location of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e6adf04-a565-42e2-98d7-195f885f6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosencountry = input(\"Enter the country of interest: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1489e8d8-5b44-49e1-ab63-dc54bfe6a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "countrydf = df[df['country'] == chosencountry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a861d8c-ad60-498b-8c8e-dd26b70f46c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(countrydf['region'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20fcabe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosenregion = input(\"Enter the region of interest: \") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791f187-9843-4044-a28e-e18ca25e3094",
   "metadata": {},
   "source": [
    "## Create a new dataframe of the weather stations in the country and region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6b7885-247d-4b44-a3b5-b4c62a952796",
   "metadata": {},
   "outputs": [],
   "source": [
    "areaofinterest = df[df[\"region\"] == chosenregion]\n",
    "print(areaofinterest.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7571855-cc0e-4473-9dce-1fab7e1b77b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "listofstations = areaofinterest['id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf8a40b-533c-4b2e-838c-314de1e4a3bc",
   "metadata": {},
   "source": [
    "## Pass in each weather station ID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab41ae",
   "metadata": {},
   "source": [
    "### Create column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79faab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18edffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in listofstations:\n",
    "    url = \"https://bulk.meteostat.net/v2/daily/\" + station + \".csv.gz\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "    # Decompress the gzip file\n",
    "    data = gzip.GzipFile(fileobj=BytesIO(response.content))\n",
    "\n",
    "    # Read in the CSV file\n",
    "    df = pd.read_csv(data, usecols=[0,1,2,3],names =['Date', 'tavg', 'tmin', 'tmax'], parse_dates =['Date'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdfe9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns for the year and month of the data\n",
    "df[\"Year\"] = df[\"Date\"].dt.year\n",
    "df[\"Month\"] = df[\"Date\"].dt.month\n",
    "print(df.info())\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c52c3a-a81f-4f5d-8424-445a496eccc0",
   "metadata": {},
   "source": [
    "## Weather station data for weather station 1211\n",
    "https://dev.meteostat.net/bulk/daily.html#endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453b4ea-c8f6-4fc5-9ecd-34c9b93e71ab",
   "metadata": {},
   "source": [
    "## Average the data by month\n",
    "First check if there is any missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a674a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "count_nan = df.isnull().sum()\n",
    "print(count_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc51d47",
   "metadata": {},
   "source": [
    "If there is missing data use `dropna` to remove those rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8a2f3-f2e1-4402-af6b-42ac1a1b1b96",
   "metadata": {},
   "source": [
    "### Plot the data for the average temperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a57f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Group by year and month, then calculate the average temperature for each month\n",
    "df = df.dropna(subset=[\"tavg\"])  # Remove rows with missing average temperature\n",
    "monthly_avg = df.groupby([\"year\", \"month\"])[\"tavg\"].mean().reset_index()\n",
    "\n",
    "# Step 4: Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "years = monthly_avg[\"year\"].unique()\n",
    "\n",
    "for year in years:\n",
    "    year_data = monthly_avg[monthly_avg[\"year\"] == year]\n",
    "    plt.plot(year_data[\"month\"], year_data[\"tavg\"], marker=\"o\", label=str(year))\n",
    "\n",
    "\n",
    "plt.title(\"Average Monthly Temperature\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Average Temperature (°C)\")\n",
    "plt.xticks(range(1, 13), [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"])\n",
    "plt.legend(title=\"Year\", loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d66b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Group by year and month, then calculate the average temperature for each month\n",
    "df = df.dropna(subset=[\"tmax\"])  # Remove rows with missing average temperature\n",
    "monthly_avg = df.groupby([\"year\", \"month\"])[\"tmax\"].mean().reset_index()\n",
    "\n",
    "# Step 4: Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "years = monthly_avg[\"year\"].unique()\n",
    "\n",
    "for year in years:\n",
    "    year_data = monthly_avg[monthly_avg[\"year\"] == year]\n",
    "    plt.plot(year_data[\"month\"], year_data[\"tmax\"], marker=\"o\", label=str(year))\n",
    "\n",
    "\n",
    "# Add horizontal lines at 23°C and 15°C\n",
    "plt.axhline(y=23, color='red', linestyle='--', linewidth=1, label='23°C')\n",
    "plt.axhline(y=15, color='blue', linestyle='--', linewidth=1, label='15°C')\n",
    "\n",
    "\n",
    "plt.title(\"Maximum Monthly Temperature\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Maximum Monthly Temperature (°C)\")\n",
    "plt.xticks(range(1, 13), [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"])\n",
    "plt.legend(title=\"Year\", loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_avg.plot(x='Date', kind='line')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f1784-5daa-440d-8fcc-3bef5f7194ba",
   "metadata": {},
   "source": [
    "## Regression and Scikit Learn\n",
    "https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b009db-c438-4cb1-986f-fabca8031ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load Data\n",
    "url = \"https://bulk.meteostat.net/v2/daily/0CNUO.csv.gz\"\n",
    "df = pd.read_csv(url, compression='gzip', usecols=[0,1,2,3], names =[\"date\", \"tavg\", \"tmin\", \"tmax\"])\n",
    "\n",
    "# Step 2: Select Relevant Columns\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 3: Prepare Features\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Features: Day of the year and year; Target: Average temperature\n",
    "X = df[['day_of_year', 'year']]\n",
    "y = df['tavg']\n",
    "\n",
    "# Step 4: Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train Model\n",
    "model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate Model\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}°C\")\n",
    "\n",
    "# Step 7: Predict for Next 5 Years\n",
    "future_years = pd.date_range(start=df['date'].max() + timedelta(days=1), \n",
    "                             periods=5 * 365, freq='D')\n",
    "future_df = pd.DataFrame({\n",
    "    'date': future_years,\n",
    "    'day_of_year': future_years.day_of_year,\n",
    "    'year': future_years.year\n",
    "})\n",
    "future_predictions = model.predict(future_df[['day_of_year', 'year']])\n",
    "future_df['predicted_tavg'] = future_predictions\n",
    "\n",
    "# Step 8: Plot Known Data and Predictions\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot historical data\n",
    "plt.plot(df['date'], df['tavg'], label='Historical Data', alpha=0.6)\n",
    "\n",
    "# Plot predictions\n",
    "plt.plot(future_df['date'], future_df['predicted_tavg'], label='Predictions', alpha=0.8)\n",
    "\n",
    "plt.title('Historical and Predicted Average Temperatures')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Temperature (°C)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Step 9: Save Predictions\n",
    "future_df.to_csv('predicted_temperatures.csv', index=False)\n",
    "print(f\"Predictions saved to 'predicted_temperatures.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c13336-0325-4cc4-9e46-cead968cff74",
   "metadata": {},
   "source": [
    "# Sci Kit Learn\n",
    "https://www.askpython.com/python/examples/weather-data-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18df94c-780c-4643-a470-959f8c41b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import cycle, islice\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7181a2-bdd6-4b9d-a9a4-48353aba4769",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9dc76-7457-4ea1-99c2-ceed3e91bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"Date\"\n",
    "tavg = \"Average air temperature in Celsius\"\n",
    "tmin = \"Minimum air temperature in Celsius\"\n",
    "tmax = \"Maximum air temperature in Celsius\"\n",
    "\n",
    "station1211 = pd.read_csv(\"https://bulk.meteostat.net/v2/daily/0CNUO.csv.gz\", usecols = [0,1,2,3], names =[date, tavg, tmin, tmax],parse_dates=[0])\n",
    "#station1211[\"id\"] = df.index + 1\n",
    "print(station1211.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d213e6-c62a-4f89-a467-992cee446990",
   "metadata": {},
   "source": [
    "## Sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb102c5-cf80-4b77-b35f-0ed476c46ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = station1211[(station1211['Date'] % 2) == 0]\n",
    "sampled_df.shape\n",
    "\n",
    "#sampled_df.set_index('Date', inplace=True)\n",
    "\n",
    "# del sampled_df['Average air temperature in Celsius']\n",
    "# del sampled_df['Maximum air temperature in Celsius']\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf863f-aef4-4244-a812-5dd695609f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = sampled_df.shape[0]\n",
    "sampled_df = sampled_df.dropna()\n",
    "A = sampled_df.shape[0]\n",
    " \n",
    "print(\"No of rows deleted: \", B-A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0019962e-765b-44c7-8dd3-329a9fd6dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Load Data\n",
    "url = \"https://bulk.meteostat.net/v2/daily/0CNUO.csv.gz\"\n",
    "df = pd.read_csv(url, compression='gzip', usecols=[0,1,2,3], names =[date, tavg, tmin, tmax])\n",
    "df.info()\n",
    "\n",
    "# Step 2: Select First Four Columns\n",
    "#df = df[0,1,2,3]\n",
    "\n",
    "# Step 3: Handle Missing Values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 4: Add Cyclical Features for Date\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['day_of_year'] = df['Date'].dt.dayofyear\n",
    "df['sin_day'] = np.sin(2 * np.pi * df['day_of_year'] / 365.0)\n",
    "df['cos_day'] = np.cos(2 * np.pi * df['day_of_year'] / 365.0)\n",
    "\n",
    "# Step 5: Prepare Data for Clustering\n",
    "features = ['tavg', 'tmin', 'tmax', 'sin_day', 'cos_day']\n",
    "X = df[features]\n",
    "\n",
    "# Step 6: Apply KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Step 7: Visualize Clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in range(kmeans.n_clusters):\n",
    "    cluster_data = df[df['cluster'] == cluster]\n",
    "    plt.scatter(cluster_data['day_of_year'], cluster_data['tavg'], label=f'Cluster {cluster}')\n",
    "    \n",
    "plt.title('KMeans Clustering of Weather Data')\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Average Temperature (°C)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3545664-8281-42a7-a005-4a0f18c3faf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2affdaa4",
   "metadata": {},
   "source": [
    "## Thermoreactive glass\n",
    "\n",
    "The data in the below has been changed to help protect the intellectual property. Many plant crops scorch and reduce their efficiency at temperatures over 23 degrees and the traditional approach has been to whitewash growhouses. Unfortunately, this white washing is not easily removed, so whitewashes are applied at the beginning of the summer and washed off at the end, this means if there is a run of poor weather in summer the plants can't be productive and if there is good weather outside of summer the plants scorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60361ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight data for days with tmax > 23 degrees and proximity to tmax < 17 degrees\n",
    "# Step 5: Identify days with tmax > 23 degrees\n",
    "df[\"hot_days\"] = df[\"tmax\"] > 23\n",
    "\n",
    "# Step 6: Identify days within 5 days of a day with tmax < 17 degrees\n",
    "df[\"cold_days\"] = df[\"tmax\"] < 17\n",
    "df[\"cold_within_5\"] = df[\"cold_days\"].rolling(window=5, min_periods=1).max().shift(-4).fillna(0).astype(bool)\n",
    "\n",
    "df[\"hot_near_cold\"] = df[\"hot_days\"] & df[\"cold_within_5\"]\n",
    "\n",
    "# Step 7: Group by month and count hot days and hot days near cold days\n",
    "highlight_summary = df.groupby(\"month\").agg(\n",
    "    total_hot_days=(\"hot_days\", \"sum\"),\n",
    "    hot_near_cold_days=(\"hot_near_cold\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "# Highlight data for days with tmax > 23 degrees and proximity to tmax < 17 degrees\n",
    "# Step 5: Identify days with tmax > 23 degrees\n",
    "df[\"hot_days\"] = df[\"tmax\"] > 23\n",
    "\n",
    "# Step 6: Identify days within 5 days of a day with tmax < 17 degrees\n",
    "df[\"cold_days\"] = df[\"tmax\"] < 17\n",
    "df[\"cold_within_5\"] = df[\"cold_days\"].rolling(window=5, min_periods=1).max().shift(-4).fillna(0).astype(bool)\n",
    "\n",
    "df[\"hot_near_cold\"] = df[\"hot_days\"] & df[\"cold_within_5\"]\n",
    "\n",
    "# Step 7: Group by year and month to count hot days and hot days near cold days\n",
    "highlight_summary = df.groupby([\"year\", \"month\"]).agg(\n",
    "    total_hot_days=(\"hot_days\", \"sum\"),\n",
    "    hot_near_cold_days=(\"hot_near_cold\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "# Step 8: Plot hot days and hot days near cold days\n",
    "plt.figure(figsize=(12, 6))\n",
    "for year in highlight_summary[\"year\"].unique():\n",
    "    year_data = highlight_summary[highlight_summary[\"year\"] == year]\n",
    "    plt.plot(year_data[\"month\"], year_data[\"total_hot_days\"], marker=\"o\", label=f\"Hot Days {year}\")\n",
    "    plt.plot(year_data[\"month\"], year_data[\"hot_near_cold_days\"], marker=\"x\", label=f\"Hot Near Cold {year}\")\n",
    "\n",
    "plt.title(\"Monthly Hot Days and Hot Near Cold Days\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of Days\")\n",
    "plt.xticks(range(1, 13), [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"])\n",
    "plt.legend(title=\"Year\", loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Step 9: Predict trends for the next five years\n",
    "# Prepare data for regression analysis\n",
    "highlight_summary[\"time_index\"] = highlight_summary[\"year\"] + (highlight_summary[\"month\"] - 1) / 12.0\n",
    "\n",
    "# Predict total_hot_days\n",
    "X = highlight_summary[[\"time_index\"]]\n",
    "y_hot_days = highlight_summary[\"total_hot_days\"]\n",
    "hot_days_model = LinearRegression().fit(X, y_hot_days)\n",
    "\n",
    "# Predict hot_near_cold_days\n",
    "y_hot_near_cold_days = highlight_summary[\"hot_near_cold_days\"]\n",
    "hot_near_cold_model = LinearRegression().fit(X, y_hot_near_cold_days)\n",
    "\n",
    "# Generate future predictions\n",
    "future_years = np.arange(highlight_summary[\"year\"].max() + 1, highlight_summary[\"year\"].max() + 6)\n",
    "future_months = np.tile(range(1, 13), len(future_years))\n",
    "future_years_repeated = np.repeat(future_years, 12)\n",
    "future_time_index = future_years_repeated + (future_months - 1) / 12.0\n",
    "\n",
    "future_X = pd.DataFrame({\"time_index\": future_time_index})\n",
    "future_hot_days = hot_days_model.predict(future_X)\n",
    "future_hot_near_cold_days = hot_near_cold_model.predict(future_X)\n",
    "\n",
    "# Combine predictions into a DataFrame\n",
    "future_predictions = pd.DataFrame({\n",
    "    \"year\": future_years_repeated,\n",
    "    \"month\": future_months,\n",
    "    \"predicted_hot_days\": future_hot_days,\n",
    "    \"predicted_hot_near_cold_days\": future_hot_near_cold_days\n",
    "})\n",
    "\n",
    "# Display predictions\n",
    "print(future_predictions)\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "for year in future_years:\n",
    "    year_data = future_predictions[future_predictions[\"year\"] == year]\n",
    "    plt.plot(year_data[\"month\"], year_data[\"predicted_hot_days\"], marker=\"o\", label=f\"Predicted Hot Days {year}\")\n",
    "    plt.plot(year_data[\"month\"], year_data[\"predicted_hot_near_cold_days\"], marker=\"x\", label=f\"Predicted Hot Near Cold {year}\")\n",
    "\n",
    "plt.title(\"Predicted Monthly Hot Days and Hot Near Cold Days\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of Days\")\n",
    "plt.xticks(range(1, 13), [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"])\n",
    "plt.legend(title=\"Year\", loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d63a85",
   "metadata": {},
   "source": [
    "[(1)]https://dev.meteostat.net/\n",
    "[(2)] https://content.ces.ncsu.edu/lettuce\n",
    "[(3)] https://drygair.com/blog/what-are-the-ideal-conditions-for-greenhouse-tomatoes/\n",
    "https://www.askpython.com/python/examples/weather-data-clustering\n",
    "\n",
    "https://esciencecenter-digital-skills.github.io/lesson-machine-learning-intro/02-data-exploration.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
